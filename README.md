# Curated Resources-For-Tranformers-LLMs-NLP
## Language Models before Transformers
  1) Add resources on Traditional NLP
  2) RNNs, LSTMs and GRUs :
     * RNNs - 4 Part Series, Denny’s Blog:
       * [Recurrent Neural Networks Tutorial, Part 1 – Introduction to RNNs](https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-1/)
       * [Recurrent Neural Networks Tutorial, Part 2 – Implementing a RNN with Python, Numpy and Theano](https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-2/)
       * [Recurrent Neural Networks Tutorial, Part 3 – Backpropagation Through Time and Vanishing Gradients](https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-3/)
       * [Recurrent Neural Network Tutorial, Part 4 – Implementing a GRU and LSTM RNN with Python and Theano](https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-4/)
     * [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) - Colah's Blog
     * [Illustrated Guide to LSTM’s and GRU’s: A step by step explanation](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)
     * [Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM)](https://www.youtube.com/watch?v=WCUNPb-5EYI) - By Brandon Rohrer


## Transformers - Everything you need to know
  1) [The Illustrated Attention](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/) - By Jay Alammar
  2) [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) - By Jay Alammar
  3) [CS25 I Stanford Seminar - Introduction to Transformers](https://www.youtube.com/watch?v=XfpMkf4rD6E) - By Andrej Karpathy
  4) Best Guide to internalize working of Transformers
     * [Visual Guide to Transformer Neural Networks - (Episode 1) Position Embeddings](https://www.youtube.com/watch?v=dichIcUZfOw&t=16s)
     * [Visual Guide to Transformer Neural Networks - (Episode 2) Multi-Head & Self-Attention](https://www.youtube.com/watch?v=mMa2PmYJlCo&t=4s)
     * [Visual Guide to Transformer Neural Networks - (Episode 3) Decoder’s Masked Attention](https://www.youtube.com/watch?v=gJ9kaJsE78k)
  5) [Transformers from Scratch](https://www.youtube.com/watch?v=QCJQG4DuHT0&list=PLTl9hO2Oobd97qfWC40gOSU8C0iu0m2l4) - CodeEmporium
     * 10 Videos series goes to the required depth needed for understanding transformers followed by a code implementation. Author has broken down the key concepts
       in a sensible manner making it easy to follow and pay attention because Attention is all you need !
  6)  [Transformer Math 101](https://blog.eleuther.ai/transformer-math/) - eleutherai

### Extras on Transformers
  * [Transformer Inference Arithmetic](https://kipp.ly/transformer-inference-arithmetic/) - kipply's blog
  * [How transformers work at inference vs training time ?](https://www.youtube.com/watch?v=IGu7ivuy1Ag) - Niels Rogge@huggingface
  * [The FLOPs Calculus of Language Model Training](https://medium.com/@dzmitrybahdanau/the-flops-calculus-of-language-model-training-3b19c1f025e4) - Dzmitry Bahdanau


## LLMs and associated tools/techniques
  * On RAG

## ML communities
  1) [ML Commmons](https://mlcommons.org/)
     * MLCommons is an Artificial Intelligence engineering consortium, built on a philosophy of open collaboration to improve AI systems. Through our collective engineering efforts with industry and academia we continually measure and improve the accuracy, safety, speed and efficiency of AI technologies–helping companies and universities around the world build better AI systems that will benefit society. 

 
