# Curated Resources-For-Tranformers-LLMs-NLP
## Language Models before Transformers
  1) Add resources on Traditional NLP
  2) RNNs, LSTMs and GRUs :
     * RNNs - 4 Part Series, Denny’s Blog:
       * [Recurrent Neural Networks Tutorial, Part 1 – Introduction to RNNs](https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-1/)
       * [Recurrent Neural Networks Tutorial, Part 2 – Implementing a RNN with Python, Numpy and Theano](https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-2/)
       * [Recurrent Neural Networks Tutorial, Part 3 – Backpropagation Through Time and Vanishing Gradients](https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-3/)
       * [Recurrent Neural Network Tutorial, Part 4 – Implementing a GRU and LSTM RNN with Python and Theano](https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-4/)
     * [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) - Colah's Blog
     * [Illustrated Guide to LSTM’s and GRU’s: A step by step explanation](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)
     * [Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM)](https://www.youtube.com/watch?v=WCUNPb-5EYI) - By Brandon Rohrer


## Transformers - Everything you need to know
  1) [The Illustrated Attention](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/) - By Jay Alammar
  2) [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) - By Jay Alammar
  3) [CS25 I Stanford Seminar - Introduction to Transformers](https://www.youtube.com/watch?v=XfpMkf4rD6E) - By Andrej Karpathy
  4) Best Guide to internalize working of Transformers
     * [Visual Guide to Transformer Neural Networks - (Episode 1) Position Embeddings](https://www.youtube.com/watch?v=dichIcUZfOw&t=16s)
     * [Visual Guide to Transformer Neural Networks - (Episode 2) Multi-Head & Self-Attention](https://www.youtube.com/watch?v=mMa2PmYJlCo&t=4s)
     * [Visual Guide to Transformer Neural Networks - (Episode 3) Decoder’s Masked Attention](https://www.youtube.com/watch?v=gJ9kaJsE78k)
  5) [Transformers from Scratch](https://www.youtube.com/watch?v=QCJQG4DuHT0&list=PLTl9hO2Oobd97qfWC40gOSU8C0iu0m2l4) - CodeEmporium
     * 10 Videos series goes to the required depth needed for understanding transformers followed by a code implementation. Author has broken down the key concepts
       in a sensible manner making it easy to follow and pay attention because Attention is all you need !
  6)  [Transformer Math 101](https://blog.eleuther.ai/transformer-math/) - eleutherai

### Extras on Transformers
  * [Transformer Inference Arithmetic](https://kipp.ly/transformer-inference-arithmetic/) - kipply's blog
  * [How transformers work at inference vs training time ?](https://www.youtube.com/watch?v=IGu7ivuy1Ag) - Niels Rogge@huggingface
  * [The FLOPs Calculus of Language Model Training](https://medium.com/@dzmitrybahdanau/the-flops-calculus-of-language-model-training-3b19c1f025e4) - Dzmitry Bahdanau


### On Quantization
  * [Quantization in Deep Learning](https://www.youtube.com/watch?v=UQlsqdwCQdc)
    * Topics covered include Types of Quantization - Uniform and Non-Uniform quantization, and further divisions of Uniform quantization such as symmetric and asymmetric quantization, dequantization, choosing the scale factor and zero point parameters for both symmetric and asymmetric quantization. Lastly, Post-training quantization or PQT and Quantization Aware Training or QAT are also covered.
  * [(GPTQ vs. GGUF vs. AWQ)](https://www.youtube.com/watch?v=mNE_d-C82lI&pp=ygUUcXVhbnRpemF0aW9uIG9mIGxsbXM%3D)

## LLMs and associated tools/techniques
  * On RAG

## ML communities
  1) [ML Commmons](https://mlcommons.org/)
     * MLCommons is an Artificial Intelligence engineering consortium, built on a philosophy of open collaboration to improve AI systems. Through our collective engineering efforts with industry and academia we continually measure and improve the accuracy, safety, speed and efficiency of AI technologies–helping companies and universities around the world build better AI systems that will benefit society. 

## Articles and Books
  1) [Prediction Machines : Simple Economics of AI](https://www.predictionmachines.ai/) - Book by Ajay Agarwal and Joshua Gans
  2) [Three Principles for Designing ML-Powered Products](https://spotify.design/article/three-principles-for-designing-ml-powered-products)
  3) [Software 2.0](https://karpathy.medium.com/software-2-0-a64152b37c35) - By Andrej Karpathy 
 


# ML Resources
  1) [Machine learning, explained - MIT Sloan as a blog post](https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained)
  2) [AWS Machine Learning Blog](https://aws.amazon.com/blogs/machine-learning/)
  3) [The 10 Best AI, Data Science and Machine Learning Podcasts](https://medium.com/startup-grind/the-10-best-ai-data-science-and-machine-learning-podcasts-d7495cfb127c)
  4) [An Introduction to Statistical Learning](https://www.statlearning.com/)
